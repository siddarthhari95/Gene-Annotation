{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading +/- Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative data sets as dataframe\n",
    "positive_data = pd.read_csv('../deep_annotator_data/positive_sample.txt', header=None, nrows=2000)\n",
    "positive_data.columns = [\"Gene\"]\n",
    "negative_data = pd.read_csv('../deep_annotator_data/negative_sample.txt', header=None, nrows=2000)\n",
    "negative_data.columns = [\"Gene\"]\n",
    "data_ = pd.concat([positive_data, negative_data])\n",
    "positive_test_data = pd.read_fwf('positive_sample_test.txt', header = None)\n",
    "positive_test_data.columns = [\"Gene\"]\n",
    "negative_test_data = pd.read_fwf('negative_sample_test.txt', header = None)\n",
    "negative_test_data.columns = [\"Gene\"]\n",
    "data_test = pd.concat([positive_test_data, negative_test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyper Parameters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 5\n",
    "window = 1\n",
    "fc_layer_size = (len(positive_data.Gene[0])-(window-1))*embedding_size\n",
    "hidden_layer_size = 100\n",
    "num_layers = 2\n",
    "epochs = 800\n",
    "lr = 0.008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate word IDs <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n"
     ]
    }
   ],
   "source": [
    "strings = set()\n",
    "def allLexicographicRecur (string, data, last, index): \n",
    "    length = len(string)\n",
    "    for i in range(length): \n",
    "        data[index] = string[i] \n",
    "        if index==last:\n",
    "            res = ''.join(data)\n",
    "            strings.add(res)\n",
    "        else: \n",
    "            allLexicographicRecur(string, data, last, index+1) \n",
    "def allLexicographic(string, n): \n",
    "    length = len(string)\n",
    "    data = [\"\"] * (length+1)\n",
    "    string = sorted(string) \n",
    "    allLexicographicRecur(string, data, window-1, 0)\n",
    "string = \"01234\"\n",
    "allLexicographic(string, window)\n",
    "strings = sorted(strings)\n",
    "vocabulary = {}\n",
    "for val, i in enumerate(strings):\n",
    "    vocabulary[i] = val\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate Word Embeddings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = nn.Embedding(len(vocabulary), 5)\n",
    "# embeddings = {}\n",
    "# def generate_embeddings():\n",
    "#     for word in vocabulary:\n",
    "#         embeddings[word] = embeds(torch.tensor(vocabulary[word], dtype=torch.long)).type(torch.LongTensor)\n",
    "\n",
    "# generate_embeddings()\n",
    "# print(embeddings['0'].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(data_sample):\n",
    "    list_of_tensors = []\n",
    "    for data in data_sample.itertuples():\n",
    "        for i in range(len(data.Gene) - window + 1):\n",
    "            if i == 0:\n",
    "                first_tensor = embeddings[data.Gene[i:i+window]]\n",
    "            else:\n",
    "                first_tensor = torch.cat((first_tensor, embeddings[data.Gene[i:i+window]]), 0)\n",
    "        list_of_tensors.append(first_tensor)\n",
    "    trainpositives = torch.stack(list_of_tensors)\n",
    "    return trainpositives\n",
    "\n",
    "# positives = generate_samples(positive_data)\n",
    "# print(positives.type())\n",
    "# negatives = generate_samples(negative_data)\n",
    "# print(negatives.type())\n",
    "# data_ = torch.cat([positives, negatives], dim=0)\n",
    "# print(data_.type())\n",
    "\n",
    "negative_labels = torch.zeros(2000, 1)\n",
    "positive_labels = torch.ones(2000, 1)\n",
    "labels_ = torch.cat([positive_labels, negative_labels], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network Layer Implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeds = nn.Embedding(len(vocabulary), 5)\n",
    "        self.lstm = nn.LSTM(fc_layer_size, fc_layer_size, num_layers)\n",
    "        self.fc1 = nn.Linear(fc_layer_size, hidden_layer_size)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.relu1 = nn.Sigmoid()\n",
    "        self.out = nn.Linear(hidden_layer_size, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = autograd.Variable(torch.randn(num_layers, 1, fc_layer_size))\n",
    "        c = autograd.Variable(torch.randn(num_layers, 1, fc_layer_size))\n",
    "        z = self.embeds(x).view((1,-1))\n",
    "#         print(z)\n",
    "        out_lstm, hn = self.lstm(z, (h, c))\n",
    "        a1 = self.fc1(z)\n",
    "        d1 = self.dropout(a1)\n",
    "        h1 = self.relu1(a1)\n",
    "        a3 = self.out(h1)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Optimizer step and loss calculation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(net.parameters(), lr, momentum=0.0)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train method </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, batch_size=1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    c = 0\n",
    "    wrong, correct = 0, 0\n",
    "    for data in data_.itertuples():\n",
    "        \n",
    "        data_batch = torch.tensor([vocabulary[data.Gene[i:i+window]] for i in range(0, len(data.Gene) - window + 1)], dtype=torch.long)\n",
    "        labels_batch = labels_[c]\n",
    "        c+=1\n",
    "        data_batch = autograd.Variable(data_batch)\n",
    "        labels_batch = autograd.Variable(labels_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Extend the side of the data_batch to adhere to LSTM layer implementation\n",
    "        # data_batch.unsqueeze_(0)\n",
    "        # data_batch = data_batch.expand(1, batch_size, fc_layer_size)\n",
    "        labels_hat = net(data_batch)\n",
    "        \n",
    "        # Compute the binary Cross Entropy Loss\n",
    "        loss = criterion(labels_hat, labels_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "        correct, wrong = get_train_accuracy(labels_hat, c, len(labels_), correct, wrong)\n",
    "    loss = sum(losses)/len(losses)\n",
    "    return loss,correct,wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train Accuracy </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_accuracy(label, index, data_size, correct, wrong):    \n",
    "    if index < data_size/2+1:\n",
    "        if label > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    else:\n",
    "        if label > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_acc = []\n",
    "    for e in range(epochs):\n",
    "        loss, c, w = train_epoch(net, opt, criterion, batch_size)\n",
    "        accuracies.append(100*(c/(c+w)))\n",
    "        losses.append(loss)\n",
    "        if e % 20 == 0:\n",
    "            print('Average Loss at epoch,',e,':',loss)\n",
    "        torch.save(net.state_dict(), 'fc_with_lstm.pt')\n",
    "        correct, wrong = load_test_model()\n",
    "        test_acc.append(100*(correct/(correct+wrong)))\n",
    "    plt.plot(losses)\n",
    "    return accuracies, test_acc\n",
    "acc,test = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x125dd62e8>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc)\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test Data Prediction </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Load the Model and test data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model():\n",
    "    #load model\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('fc_with_lstm.pt'))\n",
    "    model.eval()\n",
    "    # load data\n",
    "    labels_predicted = []\n",
    "    return test_prediction_model(data_test, labels_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Generate output labels for test predictions </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_model(data_test, labels_predicted):\n",
    "    correct, wrong = 0, 0\n",
    "    for data in data_test.itertuples():\n",
    "        data_testing = torch.tensor([vocabulary[data.Gene[i:i+window]] for i in range(0, len(data.Gene) - window + 1)], dtype=torch.long)\n",
    "        # data_test_.unsqueeze_(0)\n",
    "        # data_testing = data_test_.expand(1, batch_size, fc_layer_size)\n",
    "        labels_hat = net(data_testing)\n",
    "        labels_predicted.append(labels_hat[0])\n",
    "    for i in range(len(labels_predicted)//2 + 1):\n",
    "        if labels_predicted[i] > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    for i in range(101, len(labels_predicted)):\n",
    "        if labels_predicted[i] > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return correct,wrong\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Accuracy for Test predictions </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_accuracy(labels):\n",
    "    correct, wrong = 0, 0\n",
    "    labels_hat = labels\n",
    "    for i in range(len(labels_hat)//2 + 1):\n",
    "        if labels_hat[i] > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    for i in range(101, len(labels_hat)):\n",
    "        if labels_hat[i] > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return (correct,wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_size = 5\n",
    "fc_layer_size = (len(positive_data.Gene[0])-(window-1))*embedding_size\n",
    "hidden_layer_size = 100\n",
    "num_layers = 2\n",
    "epochs = 500\n",
    "lr = 0.02\n",
    "# train()\n",
    "# labels = []\n",
    "# test_prediction_model(data_test, labels)\n",
    "# results = test_prediction_accuracy(labels)\n",
    "# print(\"Correct Predictions:\", results[0])\n",
    "# print(\"Wrong Predictions:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
