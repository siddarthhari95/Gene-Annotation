{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyper Parameters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "embedding_size = 5\n",
    "window = 1\n",
    "positive_number = 1000\n",
    "negative_number = 1000\n",
    "hidden_layer_size = 100\n",
    "num_layers = 2\n",
    "epochs = 800\n",
    "lr = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading +/- Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = pd.read_csv('../deep_annotator_data/positive_sample.txt', header=None, nrows=positive_number)\n",
    "positive_data.columns = [\"Gene\"]\n",
    "negative_data = pd.read_csv('../deep_annotator_data/negative_sample.txt', header=None, nrows=negative_number)\n",
    "negative_data.columns = [\"Gene\"]\n",
    "data_ = pd.concat([positive_data, negative_data])\n",
    "positive_test_data = pd.read_fwf('positive_sample_test.txt', header = None)\n",
    "positive_test_data.columns = [\"Gene\"]\n",
    "negative_test_data = pd.read_fwf('negative_sample_test.txt', header = None)\n",
    "negative_test_data.columns = [\"Gene\"]\n",
    "data_test = pd.concat([positive_test_data, negative_test_data])\n",
    "\n",
    "fc_layer_size = (len(positive_data.Gene[0])-(window-1))*embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate word IDs <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n"
     ]
    }
   ],
   "source": [
    "strings = set()\n",
    "def allLexicographicRecur (string, data, last, index): \n",
    "    length = len(string)\n",
    "    for i in range(length): \n",
    "        data[index] = string[i] \n",
    "        if index==last:\n",
    "            res = ''.join(data)\n",
    "            strings.add(res)\n",
    "        else: \n",
    "            allLexicographicRecur(string, data, last, index+1) \n",
    "def allLexicographic(string, n): \n",
    "    length = len(string)\n",
    "    data = [\"\"] * (length+1)\n",
    "    string = sorted(string) \n",
    "    allLexicographicRecur(string, data, window-1, 0)\n",
    "string = \"01234\"\n",
    "allLexicographic(string, window)\n",
    "strings = sorted(strings)\n",
    "vocabulary = {}\n",
    "for val, i in enumerate(strings):\n",
    "    vocabulary[i] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate Word Embeddings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_labels = torch.zeros(positive_number, 1)\n",
    "positive_labels = torch.ones(negative_number, 1)\n",
    "labels_ = torch.cat([positive_labels, negative_labels], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network Layer Implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeds = nn.Embedding(len(vocabulary), embedding_size)\n",
    "        self.lstm = nn.LSTM(fc_layer_size, fc_layer_size, num_layers)\n",
    "        self.fc1 = nn.Linear(fc_layer_size, hidden_layer_size)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.relu1 = F.relu\n",
    "        self.out = nn.Linear(hidden_layer_size, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.embeds(x).view((1,-1))\n",
    "        h = autograd.Variable(torch.randn(num_layers, 1, fc_layer_size))\n",
    "        c = autograd.Variable(torch.randn(num_layers, 1, fc_layer_size))\n",
    "        z.unsqueeze_(0)\n",
    "        z = z.expand(1, 1, fc_layer_size)\n",
    "        out_lstm, hn = self.lstm(z, (h, c))\n",
    "        a1 = self.fc1(out_lstm)\n",
    "        d1 = self.dropout(a1)\n",
    "        h1 = self.relu1(a1)\n",
    "        a3 = self.out(h1)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Optimizer step and loss calculation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(net.parameters(), lr, momentum=0.0)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train method </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, batch_size=1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    c = 0\n",
    "    wrong, correct = 0, 0\n",
    "    for data in data_.itertuples():\n",
    "        \n",
    "        data_batch = torch.tensor([vocabulary[data.Gene[i:i+window]] for i in range(0, len(data.Gene) - window + 1)], dtype=torch.long)\n",
    "        labels_batch = labels_[c]\n",
    "        c+=1\n",
    "        data_batch = autograd.Variable(data_batch)\n",
    "        labels_batch = autograd.Variable(labels_batch)\n",
    "        opt.zero_grad()\n",
    "        labels_hat = net(data_batch)\n",
    "        loss = criterion(labels_hat, labels_batch)\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "        correct, wrong = get_train_accuracy(labels_hat, c, len(labels_), correct, wrong)\n",
    "    loss = sum(losses)/len(losses)\n",
    "    return loss,correct,wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_acc = []\n",
    "    for e in tqdm(range(epochs)):\n",
    "        loss, c, w = train_epoch(net, opt, criterion, batch_size)\n",
    "        accuracies.append(100*(c/(c+w)))\n",
    "        losses.append(loss)\n",
    "        if e % 20 == 0:\n",
    "            print('Average Loss at epoch,',e,':',loss)\n",
    "        torch.save(net.state_dict(), 'fc_with_lstm.pt')\n",
    "        correct, wrong = load_test_model()\n",
    "        test_acc.append(100*(correct/(correct+wrong)))\n",
    "    return accuracies, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train Accuracy </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_accuracy(label, index, data_size, correct, wrong):    \n",
    "    if index < data_size/2+1:\n",
    "        if label > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    else:\n",
    "        if label > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Test Accuracy </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model():\n",
    "    #load model\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('fc_with_lstm.pt'))\n",
    "    model.eval()\n",
    "    # load data\n",
    "    labels_predicted = []\n",
    "    return test_prediction_model(data_test, labels_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_model(data_test, labels_predicted):\n",
    "    correct, wrong = 0, 0\n",
    "    for data in data_test.itertuples():\n",
    "        data_testing = torch.tensor([vocabulary[data.Gene[i:i+window]] for i in range(0, len(data.Gene) - window + 1)], dtype=torch.long)\n",
    "        labels_hat = net(data_testing)\n",
    "        labels_predicted.append(labels_hat[0])\n",
    "    for i in range(len(labels_predicted)//2 + 1):\n",
    "        if labels_predicted[i] > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    for i in range(101, len(labels_predicted)):\n",
    "        if labels_predicted[i] > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return correct,wrong\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_accuracy(labels):\n",
    "    correct, wrong = 0, 0\n",
    "    labels_hat = labels\n",
    "    for i in range(len(labels_hat)//2 + 1):\n",
    "        if labels_hat[i] > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    for i in range(101, len(labels_hat)):\n",
    "        if labels_hat[i] > 0.5:\n",
    "            wrong += 1\n",
    "        else:\n",
    "            correct += 1\n",
    "    return (correct,wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d293231bddb4b0ba57d2c6b70651764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddarth/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss at epoch, 0 : 0.20894982039090246\n",
      "Average Loss at epoch, 20 : 0.027336097672459935\n",
      "Average Loss at epoch, 40 : 0.017930659443009284\n",
      "Average Loss at epoch, 60 : 0.0012133881951050683\n",
      "Average Loss at epoch, 80 : 0.0018187688321592078\n",
      "Average Loss at epoch, 100 : 0.0016468489405140972\n",
      "Average Loss at epoch, 120 : 0.00030879225454438596\n",
      "Average Loss at epoch, 140 : 9.483403942549273e-05\n",
      "Average Loss at epoch, 160 : 1.534185289601453e-05\n",
      "Average Loss at epoch, 180 : 8.779977748815782e-06\n",
      "Average Loss at epoch, 200 : 1.2972289334733488e-05\n",
      "Average Loss at epoch, 220 : 2.1675792483648593e-06\n",
      "Average Loss at epoch, 240 : 5.653464590608337e-06\n",
      "Average Loss at epoch, 260 : 2.263272140730521e-06\n",
      "Average Loss at epoch, 280 : 1.8478656053630972e-06\n",
      "Average Loss at epoch, 300 : 1.145303193347047e-06\n",
      "Average Loss at epoch, 320 : 7.505099087808275e-06\n",
      "Average Loss at epoch, 340 : 1.933587360078093e-06\n",
      "Average Loss at epoch, 360 : 3.257642030973784e-06\n",
      "Average Loss at epoch, 380 : 1.5500978570770486e-06\n",
      "Average Loss at epoch, 400 : 1.359866609291771e-06\n",
      "Average Loss at epoch, 420 : 1.6662518697430074e-06\n",
      "Average Loss at epoch, 440 : 6.683883287124103e-07\n",
      "Average Loss at epoch, 460 : 1.0726697826086707e-06\n",
      "Average Loss at epoch, 480 : 7.475441875044453e-07\n",
      "Average Loss at epoch, 500 : 1.0210111264726152e-05\n",
      "Average Loss at epoch, 520 : 9.603739595718252e-07\n",
      "Average Loss at epoch, 540 : 2.6644756105440593e-06\n"
     ]
    }
   ],
   "source": [
    "acc,test = train()\n",
    "print('Test Accuracy:', test[-1])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(test)\n",
    "plt.plot(acc)\n",
    "plt.gca().legend('Train', 'Test')\n",
    "plt.title('Train and Test Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
